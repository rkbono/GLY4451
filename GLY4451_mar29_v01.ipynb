{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff5a4a37",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/rkbono/GLY4451/blob/main/GLY4451_mar29_v01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a3bbcf",
   "metadata": {
    "id": "18a3bbcf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from IPython.display import Image\n",
    "\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    print('Running on CoLab')\n",
    "    !pip install --no-binary shapely shapely --force\n",
    "    !git clone https://github.com/rkbono/GLY4451.git\n",
    "    !pip install cartopy\n",
    "    import cartopy.crs as ccrs\n",
    "    fpath = './GLY4451/'\n",
    "else:\n",
    "    import cartopy.crs as ccrs\n",
    "    \n",
    "    print('Not running on CoLab')\n",
    "    fpath = './'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c9bc53",
   "metadata": {
    "id": "12c9bc53"
   },
   "source": [
    "# Pickle\n",
    "\n",
    "Pickle is more than a ... vegetable? It's a way to pack/unpack data into a single, compressed file. It's similar to a zip file or a matlab .mat file. You can turn python variables into pickle files and vice/versa. Just like with pickles in real life?\n",
    "\n",
    "Formally (from website):\n",
    "\n",
    "*The pickle module implements binary protocols for serializing and de-serializing a Python object structure. “Pickling” is the process whereby a Python object hierarchy is converted into a byte stream, and “unpickling” is the inverse operation, whereby a byte stream (from a binary file or bytes-like object) is converted back into an object hierarchy. Pickling (and unpickling) is alternatively known as “serialization”, “marshalling,” 1 or “flattening”; however, to avoid confusion, the terms used here are “pickling” and “unpickling”.*\n",
    "\n",
    "https://docs.python.org/3/library/pickle.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a013aa1",
   "metadata": {
    "id": "7a013aa1"
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc12234",
   "metadata": {
    "id": "ecc12234"
   },
   "outputs": [],
   "source": [
    "# with open(fpath+\"Datasets/BedMachine_Antarctica.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(ant_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af7de14",
   "metadata": {
    "id": "1af7de14"
   },
   "outputs": [],
   "source": [
    "with open(fpath+\"Datasets/BedMachine_Antarctica.pkl\", \"rb\") as f:\n",
    "    ant_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c70fd92",
   "metadata": {
    "id": "3c70fd92"
   },
   "source": [
    "# NetCDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92d4653",
   "metadata": {
    "id": "e92d4653"
   },
   "source": [
    "From wiki:\n",
    "\n",
    "*NetCDF (Network Common Data Form) is a set of software libraries and self-describing, machine-independent data formats that support the creation, access, and sharing of array-oriented scientific data.*\n",
    "\n",
    "*The netCDF libraries support multiple different binary formats for netCDF files. All formats are \"self-describing\". This means that there is a header which describes the layout of the rest of the file, in particular the data arrays, as well as arbitrary file metadata in the form of name/value attributes. The format is platform independent. The data are stored in a fashion that allows efficient subsetting.*\n",
    "\n",
    "https://en.wikipedia.org/wiki/NetCDF\n",
    "\n",
    "Very common for sharing gridded data, datasets with very rigid requirements, etc., and found within most modeling communities. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab031e5",
   "metadata": {
    "id": "fab031e5"
   },
   "source": [
    "We aren't going to actually *use* the package since the datasets are typically too large to be practical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b4a923",
   "metadata": {
    "id": "71b4a923"
   },
   "outputs": [],
   "source": [
    "# import netCDF4 as nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1b59c5",
   "metadata": {
    "id": "ed1b59c5"
   },
   "outputs": [],
   "source": [
    "# ant = nc.Dataset(antPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527ebe0d",
   "metadata": {
    "id": "527ebe0d"
   },
   "source": [
    "## Antarctica\n",
    "\n",
    "This ice data looks fun -- ice surface and bedrock heights for Antarctica at 500 m spacing. See the details from the CDF file below. I've gone ahead and downsampled it by 20x to 10 km resolution so it doesn't melt your computer. If you'd like the full file, ask - the file is ~1 gb but it uses ~6 or so in RAM from my testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc8fd1d",
   "metadata": {
    "id": "cfc8fd1d"
   },
   "source": [
    "`<class 'netCDF4._netCDF4.Dataset'>\n",
    "root group (NETCDF4_CLASSIC data model, file format HDF5):\n",
    "    Conventions: CF-1.7\n",
    "    Title: BedMachine Antarctica\n",
    "    Author: Mathieu Morlighem\n",
    "    version: 03-Jun-2022 (v3.4)\n",
    "    nx: 13333.0\n",
    "    ny: 13333.0\n",
    "    Projection: Polar Stereographic South (71S,0E)\n",
    "    proj4: +init=epsg:3031\n",
    "    sea_water_density (kg m-3): 1027.0\n",
    "    ice_density (kg m-3): 917.0\n",
    "    xmin: -3333000\n",
    "    ymax: 3333000\n",
    "    spacing: 500\n",
    "    no_data: -9999.0\n",
    "    license: No restrictions on access or use\n",
    "    Data_citation: Morlighem M. et al., (2019), Deep glacial troughs and stabilizing ridges unveiled beneath the margins of the Antarctic ice sheet, Nature Geoscience (accepted)\n",
    "    Notes: Data processed at the Department of Earth System Science, University of California, Irvine\n",
    "    dimensions(sizes): x(13333), y(13333)\n",
    "    variables(dimensions): |S1 mapping(), int32 x(x), int32 y(y), int8 mask(y, x), float32 firn(y, x), float32 surface(y, x), float32 thickness(y, x), float32 bed(y, x), int16 errbed(y, x), int8 source(y, x), int8 dataid(y, x), int16 geoid(y, x)\n",
    "    groups: `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8adcd5d",
   "metadata": {
    "id": "f8adcd5d"
   },
   "outputs": [],
   "source": [
    "ant_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d4db67",
   "metadata": {
    "id": "75d4db67"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18,4))\n",
    "ax = fig.add_subplot(1,3,1,projection=ccrs.SouthPolarStereo())\n",
    "\n",
    "ax.coastlines()\n",
    "ax.gridlines()\n",
    "\n",
    "mline = int(ant_data['mx'].shape[0]/2)\n",
    "\n",
    "ax.plot(ant_data['mx'][mline,:],ant_data['my'][mline,:],'-r')\n",
    "ax.plot(ant_data['mx'][:,mline],ant_data['my'][:,mline],'-r')\n",
    "\n",
    "ax = fig.add_subplot(1,3,2)\n",
    "ax.plot(ant_data['mx'][mline,:],ant_data['bedrock'][mline,:],'-r',label='bedrock')\n",
    "ax.plot(ant_data['mx'][mline,:],ant_data['icesurf'][mline,:],'-b',label='ice')\n",
    "ax.set_title('East-West')\n",
    "\n",
    "ax = fig.add_subplot(1,3,3)\n",
    "ax.plot(ant_data['my'][:,mline],ant_data['bedrock'][:,mline],'-r',label='bedrock')\n",
    "ax.plot(ant_data['my'][:,mline],ant_data['icesurf'][:,mline],'-b',label='ice')\n",
    "ax.set_title('North-South')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2362636b",
   "metadata": {
    "id": "2362636b"
   },
   "source": [
    "# Challenge #1\n",
    "\n",
    "Given the dataset above on Antarctic ice and bedrock heights, estimate the mass of the Antarctic ice sheet to a reasonable degree. Explain how you reached an answer and what assumptions were needed. If you did not have more data, but infinite time and perfect knowledge of how to calculate it, what are some further tweaks you could make to your estimate? I was able to get within 2% of the reported volume with one line of code. With more code, it could be closer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da79b8dd",
   "metadata": {
    "id": "da79b8dd"
   },
   "source": [
    "For inspiration, here are some starter images that you can make as well with the gridded datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772480c2",
   "metadata": {
    "id": "772480c2",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Image(fpath+'Figures/antarctica_maps.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef474b38",
   "metadata": {
    "id": "ef474b38"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3dd95868",
   "metadata": {
    "id": "3dd95868"
   },
   "source": [
    "# Seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8a8446",
   "metadata": {
    "id": "0a8a8446"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88852c8",
   "metadata": {
    "id": "e88852c8"
   },
   "source": [
    "From Seaborn Introduction:\n",
    "\n",
    "*Seaborn is a library for making statistical graphics in Python. It builds on top of matplotlib and integrates closely with pandas data structures.*\n",
    "\n",
    "*Seaborn helps you explore and understand your data. Its plotting functions operate on dataframes and arrays containing whole datasets and internally perform the necessary semantic mapping and statistical aggregation to produce informative plots.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4033c114",
   "metadata": {
    "id": "4033c114"
   },
   "source": [
    "## Sea ice data\n",
    "Let's get some good-enough data -- seaborn has some built in datasets which we will use to our advantage. These are not rated for science - get real data for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75394084",
   "metadata": {
    "id": "75394084"
   },
   "outputs": [],
   "source": [
    "dfIce = sns.load_dataset('seaice')\n",
    "dfIce.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd7703b",
   "metadata": {
    "id": "8fd7703b"
   },
   "source": [
    "This dataset contains northern hemisphere sea ice extent (in millions of km) recorded on ~2-day frequency. Let's play around with it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc85451f",
   "metadata": {
    "id": "cc85451f"
   },
   "source": [
    "Let's use the Date column as an index. If we convert it to pandas DatetimeIndex type, that'll unlock some neat indexing abilities. https://pandas.pydata.org/docs/reference/api/pandas.DatetimeIndex.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e320d6",
   "metadata": {
    "id": "65e320d6"
   },
   "outputs": [],
   "source": [
    "dfIce['Date'] = pd.to_datetime(dfIce['Date'])\n",
    "dfIce.set_index('Date',inplace=True)\n",
    "dfIce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cec27b6",
   "metadata": {
    "id": "8cec27b6"
   },
   "outputs": [],
   "source": [
    "dfIce.loc['2005-03']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a580476a",
   "metadata": {
    "id": "a580476a"
   },
   "outputs": [],
   "source": [
    "sns.lineplot(data=dfIce,x=dfIce.index.dayofyear,y='Extent',hue=dfIce.index.year)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fedc60",
   "metadata": {
    "id": "71fedc60"
   },
   "source": [
    "Now let's add some \"helper\" columns for later. We'll extract those useful datetime attributes and store them as columns. It might be overkill, but it'll give us some freedom to explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dd6b1b",
   "metadata": {
    "id": "b7dd6b1b"
   },
   "outputs": [],
   "source": [
    "dfIce['year'] = dfIce.index.year\n",
    "dfIce['month'] = dfIce.index.month\n",
    "dfIce['dayofyear'] = dfIce.index.dayofyear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30075926",
   "metadata": {
    "id": "30075926"
   },
   "source": [
    "### Pandas tangent - groupby\n",
    "\n",
    "groupby is a really powerful tool that's also a pain to use. https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html\n",
    "\n",
    "Pandas will split up a dataframe based on a column of categorical data. It returns an object that functions like a dictionary, with each key referring to the category and the value being a dataframe containing just those rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f730c7",
   "metadata": {
    "id": "d3f730c7",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print out the mean ice extent by month\n",
    "\n",
    "month_names = {1:'Jan',2:'Feb',3:'Mar',4:'Apr',5:'May',6:'Jun',7:'Jul',8:'Aug',9:'Sep',10:'Oct',11:'Nov',12:'Dec'}\n",
    "for idx,grp in dfIce.groupby('month'):\n",
    "    print('%s: %.2fe6 km2'%(month_names[idx],grp['Extent'].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb235a6",
   "metadata": {
    "id": "adb235a6"
   },
   "source": [
    "groupby can be paired with another pandas tool, cut, to group numerical data into bins. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1384d98",
   "metadata": {
    "id": "e1384d98"
   },
   "outputs": [],
   "source": [
    "# define decade boundaries\n",
    "age_edges = np.arange(1980,2020+1e-5,10)\n",
    "\n",
    "# pd.cut will bin the given column, here \"year\", using the provided bins/edges, here \"age_edges\"\n",
    "dfIce['decade'] = pd.cut(dfIce['year'],age_edges,right=False)\n",
    "display(dfIce.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c627a524",
   "metadata": {
    "id": "c627a524"
   },
   "source": [
    "Note how the decade column is presented. pd.cut will define a new datatype, an \"Interval\". Parentheses indicate exclusive, brackets indicate inclusive. By default, pd.cut will INCLUDE the right edge and EXCLUDE the left edge. Here, since we generally count the 0th year as part of the decade (eg, 90s), I set the argument \"right\" to false, which reverses which edge is included/excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bd9241",
   "metadata": {
    "id": "d6bd9241"
   },
   "outputs": [],
   "source": [
    "# now lets apply groupby by decade, and we'll skip straight to some aggregate statistic\n",
    "dfIce[['Extent','decade']].groupby('decade').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a5e4ba",
   "metadata": {
    "id": "e1a5e4ba"
   },
   "source": [
    "## Some plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c30a29",
   "metadata": {
    "id": "a3c30a29"
   },
   "outputs": [],
   "source": [
    "sns.lineplot(data=dfIce['Extent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5676e983",
   "metadata": {
    "id": "5676e983"
   },
   "outputs": [],
   "source": [
    "sns.lineplot(data=dfIce.loc[dfIce.index.year>=2000,'Extent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb69a9c",
   "metadata": {
    "id": "cfb69a9c"
   },
   "outputs": [],
   "source": [
    "sns.boxplot(data=dfIce,x='month',y='Extent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c8c0a4",
   "metadata": {
    "id": "88c8c0a4"
   },
   "outputs": [],
   "source": [
    "sns.violinplot(data=dfIce,x='month',y='Extent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784c3e00",
   "metadata": {
    "id": "784c3e00"
   },
   "outputs": [],
   "source": [
    "sns.regplot(data=dfIce,x='year',y='Extent', x_estimator=np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0840eaa1",
   "metadata": {
    "id": "0840eaa1"
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1)\n",
    "sns.kdeplot(ax=ax,data=dfIce,x='Extent',hue='decade',fill=True,legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bac20ee",
   "metadata": {
    "id": "6bac20ee"
   },
   "source": [
    "# Challenge #2\n",
    "\n",
    "Using the sea ice dataset and seaborn plotting module, describe (with plots) how sea ice extent has changed on the following timescales and/or intervals:\n",
    "1. Annual\n",
    "2. Monthly\n",
    "2. Weekly\n",
    "3. Moving five-year average\n",
    "\n",
    "Use a different seaborn plot type for each timescale/interval. For each timescale/interval, quantify and/or visualize the variation in the dataset. The exact feature you present is not important, I'd just like to see that you played around a little bit with seaborn. *Be creative!*\n",
    "\n",
    "***These should be \"presentation\" or \"publication\" quality.*** More so than most of the other figures you have made in this class, I want you to also focus on the clarity, impact, and aesthetic quality of these figures. By now you should start getting a feel for how to style a figure. Use subplots and titles as appropriate, label axes, pick clear and logical colors. \n",
    "\n",
    "\n",
    "### Bonus challenge (extra credit)\n",
    "Which day of the year has the most variation in sea ice extent?\n",
    "\n",
    "(Can you answer the above question using only one line of code?)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce483cef",
   "metadata": {
    "id": "80689162"
   },
   "source": [
    "## Seaborn and Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b66859",
   "metadata": {},
   "source": [
    "These can play nicely together but it can take a little diligence. Let's play with another databased of magnetic field strength data, www.PINTdb.org, which is maintained by some guy. This is a medium sized excel table with magnetic field strength and associated meta-data reported at the site-mean level. Let's load it up and take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d0b0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPINT = pd.read_excel(fpath+'Datasets/PINTv811.xlsx')\n",
    "dfPINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525791ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPINT.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddc1167",
   "metadata": {},
   "source": [
    "We can see column descriptors here: http://pintdb.org/data/PINT_column_headings_v800.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c582c6b",
   "metadata": {},
   "source": [
    "Let's make a site map with symbols colored by age and size by QPI (a qualitative score on the presumed robustness of the data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61abd31",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# since the dataset is dominated by Cenozoic data but extends into the Hadean\n",
    "# let's look at log(age) to see more of a spread\n",
    "\n",
    "dfPINT['logAGE'] = np.log10(dfPINT['AGE'])\n",
    "\n",
    "fig = plt.figure(figsize=(8,4))\n",
    "ax = fig.add_subplot(1,1,1,projection=ccrs.Robinson())\n",
    "\n",
    "ax.set_global()\n",
    "ax.stock_img()\n",
    "\n",
    "hh = sns.scatterplot(ax=ax,data=dfPINT,x='SLONG',y='SLAT',\n",
    "                     hue='logAGE',size='QPI',legend='brief',\n",
    "                     palette='turbo',\n",
    "                     transform = ccrs.PlateCarree()\n",
    "                    )\n",
    "ax.legend(loc='center left',bbox_to_anchor=(1.05,0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c112db",
   "metadata": {},
   "source": [
    "What about shape of the field? We can get a sense of morphology based on the expected intensity-latitude relationship due to the dipole assumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924ef38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,4))\n",
    "ax = fig.add_subplot(1,1,1,projection=ccrs.Robinson())\n",
    "\n",
    "ax.set_global()\n",
    "ax.coastlines()\n",
    "\n",
    "hh = sns.scatterplot(ax=ax,data=dfPINT.loc[dfPINT['AGE']<=10],x='SLONG',y='SLAT',\n",
    "                     hue='B',legend='brief',s=300,\n",
    "                     palette='turbo',\n",
    "                     transform = ccrs.PlateCarree()\n",
    "                    )\n",
    "ax.legend(loc='center left',bbox_to_anchor=(1.05,0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c18de0",
   "metadata": {},
   "source": [
    "Hmm, not enough coverage and too much variability. Let's bin by latitude."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe7b2a8",
   "metadata": {},
   "source": [
    "Lots of scatter, can we get a trend?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bd721c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=dfPINT.loc[dfPINT['AGE']<=10],x='SLAT',y='B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6110fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPINT['lbin'] = pd.cut(dfPINT['SLAT'],bins=np.arange(-90,90.1,10))\n",
    "\n",
    "fig = plt.figure(figsize=(12,4))\n",
    "ax = fig.subplots(1,1)\n",
    "\n",
    "sns.barplot(ax=ax,data=dfPINT.loc[dfPINT['AGE']<=10],x='lbin',y='B')\n",
    "\n",
    "ax.set_xticklabels(ax.get_xticklabels(),rotation=90);\n",
    "ax.set_xlabel('Latitude Bin');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6246d2d6",
   "metadata": {},
   "source": [
    "Okay, clear trend in the Northern hemi. Southern hemi looks undersampled."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d1f973",
   "metadata": {},
   "source": [
    "Why are we only looking at the most recent 10 million years?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc04395",
   "metadata": {},
   "source": [
    "# Challenge #3\n",
    "\n",
    "Using the PINTdb, describe the distribution of field strength data from the Mesozoic. Consider sampling, strengths, quality, rock types, etc. What are some fundamental differences in the **geology** (think geography) between the Mesozoic and the recent Cenozoic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bff530c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
